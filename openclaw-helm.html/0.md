OpenClaw is an open-source AI agent that connects to messaging platforms and acts autonomously. This post covers deploying it on Kubernetes with a Helm chart that supports ArgoCD GitOps, config merge modes, declarative skill installation, network policy isolation, and secrets management.

If you've been anywhere near Hacker News in the past few weeks, you've probably seen OpenClaw. The project, originally named Clawdbot, then briefly Moltbot before settling on its current name, has exploded in popularity since late last year. It crossed 100,000 GitHub stars and attracted around 2 million visitors in a single week. The hype got so intense that [Cloudflare's stock jumped 14%](https://www.reuters.com/business/cloudflare-surges-viral-ai-agent-buzz-lifts-expectations-2026-01-27/) because people were using [Cloudflare Tunnel](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/) to host the tool. Cloudflare even used the opportunity to come up with [Moltworker](https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/), a proof of concept that runs OpenClaw on their Developer Platform using Sandboxes, Browser Rendering, and R2 storage.

What makes OpenClaw interesting isn't just that it's another AI assistant. It connects to your messaging platforms (WhatsApp, Telegram, Signal, Slack, Discord, and more) and does things autonomously. It's less chatbot, more autonomous agent. Give it a task, let it run, wake up to results.

### Security Considerations

OpenClaw's power comes with real security risks. The tool has shell access, reads your files, and processes untrusted input from emails and web content. Security researchers have identified several concerns:

- [Palo Alto Networks](https://research.aimultiple.com/moltbot/) called it a "lethal trifecta": access to private data, exposure to untrusted content, and external communication ability
- [Cisco found](https://blogs.cisco.com/ai/personal-ai-agents-like-openclaw-are-a-security-nightmare) that 26% of third-party skills contain vulnerabilities, including data exfiltration
- [Vectra AI](https://www.vectra.ai/blog/clawdbot-to-moltbot-to-openclaw-when-automation-becomes-a-digital-backdoor) documented prompt injection attacks that can achieve remote code execution
- [The Register](https://www.theregister.com/2026/01/27/clawdbot_moltbot_security_concerns/) reported hundreds of exposed instances found via Shodan, some with no authentication

Running OpenClaw in Kubernetes provides meaningful isolation compared to running it directly on your workstation. Container isolation, network policies, and resource limits contain the blast radius if something goes wrong. Your host filesystem, credentials, and other workloads remain protected behind namespace boundaries. For anyone considering OpenClaw in a work context or handling sensitive data, Kubernetes is the safer option.

That said, container isolation and network policies don't make OpenClaw completely safe. The tool still processes untrusted input and executes code inside the container. If a prompt injection leads to code execution, the damage is contained to the pod and whatever network access you've allowed, which is a lot better than having it run on your workstation with access to everything. The goal is minimizing blast radius, not eliminating risk entirely. Treat these measures as layers of defense that make exploitation significantly harder, not as a guarantee that nothing can go wrong.

I run Kubernetes for everything in my homelab. When I saw OpenClaw gaining traction, I realized there wasn't a Helm chart available for deploying it. So I built one. Everything in my homelab is declarative, version-controlled in Git, and managed through ArgoCD, that's just how I operate.

This post walks through deploying OpenClaw on Kubernetes using [my Helm chart](https://github.com/serhanekicii/openclaw-helm), also available on [Artifact Hub](https://artifacthub.io/packages/helm/openclaw-helm/openclaw).

### Deployment Architecture

The setup is straightforward:

- **OpenClaw** runs as a single-replica Deployment (it cannot scale horizontally by design)
- **Sidecars and init containers** include a Chromium browser for automation, an init-config container for initializing configuration (with merge or overwrite modes), and an init-skills container for declaratively installing ClawHub skills and runtime dependencies
- **Configuration** is stored in a ConfigMap using JSON5 format
- **Persistent storage** keeps workspace data, sessions, and application state
- **Secrets** are managed externally (Vault recommended, but optional)

The Helm chart is built on the [bjw-s app-template](https://github.com/bjw-s/helm-charts).

#### GitOps Behavior

The chart supports two config modes, controlled by the `configMode` value:

- **merge** (default): Helm values are deep-merged with the existing config on the PVC at each restart. Runtime changes made through the web UI (paired devices, settings tweaks, and so on) persist across pod restarts. Your Helm values act as defaults and overrides, but they don't wipe what's already there.
- **overwrite**: strict GitOps. The config file is replaced wholesale on every restart, so the source of truth is purely your Helm values and manifests in Git. Any changes made through the UI are ephemeral.

If you want full declarative control with no drift, set `configMode: overwrite`. If you prefer a more forgiving setup where UI changes survive restarts, leave the default.

If you use merge mode with ArgoCD, you'll want to tell ArgoCD to ignore diffs on the ConfigMap so it doesn't fight with runtime config on the PVC:

```yaml
spec:
  ignoreDifferences:
    - group: ""
      kind: ConfigMap
      name: openclaw
      jsonPointers:
        - /data
```

### Quick Start with Helm

If you just want to test the deployment quickly, you can install directly with Helm.

Add the repository and grab the default values:

```bash
helm repo add openclaw https://serhanekicii.github.io/openclaw-helm
helm repo update
helm show values openclaw/openclaw > values.yaml
```

Edit `values.yaml` to set your trusted proxies, model provider, timezone, and channels. At minimum, you'll want to configure the `trustedProxies` list and your messaging channel.

Create a namespace and secret for your API keys:

```bash
kubectl create namespace openclaw

kubectl create secret generic openclaw-env-secret \
  -n openclaw \
  --from-literal=ANTHROPIC_API_KEY=your-api-key \
  --from-literal=OPENCLAW_GATEWAY_TOKEN=your-gateway-token
```

Add the secret reference to your `values.yaml`:

```yaml
app-template:
  controllers:
    main:
      containers:
        main:
          envFrom:
            - secretRef:
                name: openclaw-env-secret
```

Install and verify:

```bash
helm install openclaw openclaw/openclaw -n openclaw -f values.yaml
kubectl get pods -n openclaw
kubectl logs -n openclaw deployment/openclaw
```

To uninstall:

```bash
helm uninstall openclaw -n openclaw
```

### GitOps Setup with ArgoCD

If you're new to ArgoCD, start with their [getting started guide](https://argo-cd.readthedocs.io/en/stable/getting_started/). The basic idea: you define your desired state in a Git repository, and ArgoCD continuously reconciles your cluster to match.

#### Umbrella Charts

For managing applications in a GitOps repository, I use the [umbrella chart pattern](https://helm.sh/docs/howto/charts_tips_and_tricks/#complex-charts-with-many-dependencies). Instead of pointing ArgoCD directly at a remote Helm repository, you create a local chart that wraps the upstream chart as a dependency. This gives you a clean structure where each application lives in its own directory.

Start by creating the directory structure:

```bash
mkdir -p workloads/my-cluster/openclaw/crds
cd workloads/my-cluster/openclaw
```

```text
workloads/
└── my-cluster/
    └── openclaw/
        ├── Chart.yaml
        ├── values.yaml
        └── crds/
            └── vault-secret.yaml
```

Create `Chart.yaml` to declare the upstream chart as a dependency:

```yaml
apiVersion: v2
name: openclaw
description: OpenClaw deployment for my-cluster
type: application
version: 1.0.0
appVersion: "2026.2.6"

dependencies:
  - name: openclaw
    version: 1.3.7
    repository: https://serhanekicii.github.io/openclaw-helm
```

Grab the default values from the upstream chart:

```bash
helm repo add openclaw https://serhanekicii.github.io/openclaw-helm
helm repo update
helm show values openclaw/openclaw > values.yaml
```

Since this is an umbrella chart, all values need to be nested under the dependency name. Edit `values.yaml` and wrap everything under `openclaw:`:

```yaml
openclaw:
  app-template:
    controllers:
      main:
        containers:
          main:
            envFrom:
              - secretRef:
                  name: openclaw-env-secret
    # ... rest of your configuration
```

Update the `openclaw.json` section with your settings: trusted proxies, model provider, timezone, channels, and so on.

Before deploying, verify your chart renders correctly:

```bash
helm dependency build
helm template openclaw . --debug
```

Create an ArgoCD Application manifest:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: openclaw
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/your-gitops-repo.git
    targetRevision: HEAD
    path: workloads/my-cluster/openclaw
  destination:
    server: https://kubernetes.default.svc
    namespace: openclaw
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

With `automated` sync enabled, ArgoCD will detect changes and deploy automatically. Any drift gets corrected.

This pattern works well with [ApplicationSets](https://argo-cd.readthedocs.io/en/latest/user-guide/application-set/) too. You can have ArgoCD automatically discover and deploy any chart in the `workloads/` directory based on path patterns. If you want a broader overview of managing multiple applications, the [App of Apps pattern](https://argo-cd.readthedocs.io/en/latest/operator-manual/cluster-bootstrapping/#app-of-apps-pattern-alternative) is worth looking at as well. For more on ArgoCD with Helm charts, see their [Helm documentation](https://argo-cd.readthedocs.io/en/stable/user-guide/helm/).

### Secrets Management

OpenClaw needs API keys and tokens to function. How you manage these depends on your setup.

If you run HashiCorp Vault (or its Linux Foundation fork [OpenBao](https://openbao.org/)), the [Vault Secrets Operator](https://developer.hashicorp.com/vault/docs/platform/k8s/vso) handles syncing secrets to Kubernetes. Store your credentials in Vault at a path like `secret/openclaw/env`, then create a VaultStaticSecret. In the umbrella chart pattern, I keep these manifests in a `crds/` directory alongside the chart. ArgoCD will apply them before the Helm release:

```yaml
apiVersion: secrets.hashicorp.com/v1beta1
kind: VaultStaticSecret
metadata:
  name: openclaw-env-secret
  namespace: openclaw
spec:
  vaultAuthRef: default
  mount: secret
  path: my-cluster/openclaw/env
  type: kv-v2
  destination:
    name: openclaw-env-secret
    create: true
```

If you're not running Vault, a native Kubernetes Secret works. You can put it in the same `crds/` directory:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: openclaw-env-secret
  namespace: openclaw
type: Opaque
stringData:
  ANTHROPIC_API_KEY: "your-api-key-here"
  OPENCLAW_GATEWAY_TOKEN: "your-gateway-token-here"
```

Plain Kubernetes Secrets are base64-encoded, not encrypted at rest unless you've configured encryption.

### Networking and Access

OpenClaw needs to be reachable for its web UI and potentially for webhook integrations with messaging platforms.

For external access, I recommend [Cloudflare Tunnel](https://github.com/cloudflare/helm-charts/tree/main/charts/cloudflare-tunnel). The cloudflared daemon runs inside your cluster as a Deployment, and no inbound ports need to be opened on your firewall.

For homelab use where you don't need external access, keep OpenClaw internal. Access it via your local network or VPN. Don't expose services unnecessarily.

If you're exposing services within your cluster, [Gateway API](https://gateway-api.sigs.k8s.io/) is the way forward. [Retirement of ingress-nginx](https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/) announced earlier last year, now's a good time to migrate to Gateway API. Here's a minimal HTTPRoute example:

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: openclaw
  namespace: openclaw
spec:
  parentRefs:
    - name: main-gateway
      namespace: gateway-system
  hostnames:
    - "openclaw.example.internal"
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /
      backendRefs:
        - name: openclaw
          port: 18789
```

For TLS termination and DNS automation (using tools like [cert-manager](https://cert-manager.io/) and [external-dns](https://github.com/kubernetes-sigs/external-dns)), that's a topic that deserves its own post. For now, just know the Helm chart exposes the service on port 18789 and you can wire it up however fits your setup.

### Network Policy Isolation

Given that OpenClaw has shell access, reads files, and processes untrusted input, network policies add an important layer of defense. Even if the application gets compromised, network policies limit what an attacker can reach from within the pod.

The Helm chart includes a network policy that's disabled by default. Enable it in your values:

```yaml
openclaw:
  app-template:
    networkpolicies:
      main:
        enabled: true
```

The default policy allows:

- **Ingress** from the `gateway-system` namespace on port 18789
- **Egress** to kube-dns for DNS resolution
- **Egress** to all public internet IPs (blocks RFC1918, link-local, and CGN ranges)

This means OpenClaw can reach external APIs (Anthropic, OpenAI, messaging platforms) but cannot access your internal services, databases, or other workloads. The blast radius of a compromise stays contained.

Adjust the ingress source to match your setup. If you're using ingress-nginx instead of Gateway API:

```yaml
openclaw:
  app-template:
    networkpolicies:
      main:
        enabled: true
        rules:
          ingress:
            - from:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: ingress-nginx
              ports:
                - protocol: TCP
                  port: 18789
```

If you need OpenClaw to reach specific internal services (Vault, a local LLM, internal APIs), add explicit egress rules:

```yaml
openclaw:
  app-template:
    networkpolicies:
      main:
        enabled: true
        rules:
          egress:
            # ... keep existing rules ...
            # Vault for secrets
            - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: vault
              ports:
                - protocol: TCP
                  port: 8200
            # Local Ollama instance
            - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: ollama
              ports:
                - protocol: TCP
                  port: 11434
```

Make sure your CNI plugin supports network policies (Calico, Cilium, etc.), otherwise the policies will be created but not enforced.

### Post-Installation: Device Pairing

Once the pod is running, you need to pair your device with OpenClaw.

Access the web UI at `https://openclaw.example.internal/` and enter your Gateway Token in Settings. If you're using Vault, this is the value stored at your configured path.

Click "Connect" to initiate a device pairing request. The request needs to be approved from within the pod:

```bash
# List pending devices
kubectl exec -n openclaw deployment/openclaw \
  --context my-cluster \
  -- node dist/index.js devices list

# Approve the device
kubectl exec -n openclaw deployment/openclaw \
  --context my-cluster \
  -- node dist/index.js devices approve <REQUEST_ID>
```

Reconnect in the UI and you should now have an active session.

Congrats, now you have a functional OpenClaw deployment in your Kubernetes setup.
